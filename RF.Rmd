---
title: "R Notebook"
output: html_notebook
---

# Random Forest

```{r}
library (caret)
library(ranger)
```

```{r}
data <- read.csv("train.csv")
test_df <- read.csv("test.csv")

data$label <- as.factor(data$label)
summary(data$label)

dim(train)
View(test_df)
```

## División conjunto test y train

```{r}
set.seed(33)

train_perc <- 0.75
train_index <- createDataPartition(data$label, p=train_perc, list=FALSE)

data_train <- data[train_index,]
data_test <- data[-train_index,]
```

## Random Forest

El segundo algoritmo que vamos a utilizar es el de Random Forest, que consiste en construir múltiples árboles de decisión durante el entrenamiento y combinar sus resultados para obtener predicciones más precisas.

Para ello, vamos a utilizar la librería **ranger**, que es una versión optimizada de **randomForest**, con los siguientes parámetros:

-   *num.trees=50*: El número de árboles que hemos indicado es 50, pues al incrementar este valor el tiempo de ejecución crece significativamente, mientras que el accuracity se muestra prácticamente invariable.

-   *importance="impurity"*: este parámetro se utiliza para indicar si se deben calcular las importancias de las variables. Su valor por defecto es "none". Sin embargo, para nuestro modelo le hemos indicado que calcule la importancia de las variables basándose en la medida de impureza de Gini o ganancia de varianza.

```{r}
rf <- ranger(label~., data=data_train, num.trees=50, importance="impurity")
save(rf, file="random_forest.RData") # Guardo mi modelo
# Para volver a usarlo, descomentar
#rf_saved <- load("random_forest.RData")
print(rf)
#importance(rf)
```

```{r}
prediction_rf<- predict(rf, data_test)

confussion_matrix_rf <- table(data_test$label, prediction_rf$predictions)
confussion_matrix_rf
```

```{r}
accuracy_rf <- mean(prediction_rf$predictions == data_test$label)
cat("Accuracy with random forest:", accuracy_rf) # 0.96
```
